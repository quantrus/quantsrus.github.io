<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Quants R Us</title>
    <link>https://quantsrus.github.io/post/</link>
    <description>Recent content in Posts on Quants R Us</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2019 15:50:22 +0200</lastBuildDate>
    
	<atom:link href="https://quantsrus.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Julia and Python for the RBF collocation of a 2D PDE with multiple precision arithmetic</title>
      <link>https://quantsrus.github.io/post/rbf_collocation_of_2d_pde_and_precision/</link>
      <pubDate>Fri, 24 May 2019 15:50:22 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/rbf_collocation_of_2d_pde_and_precision/</guid>
      <description>I was curious about using Julia, mainly to use the Arb library for arbitrary precision linear algebra. It is written by the author of Python mpmath library, but its principle is quite different, and from the author&amp;rsquo;s blog, it is supposed to be much faster. Then, I also found python + numpy + scipy not so great at building large sparse matrices. It seemed relatively slow to me, numba does not work with scipy sparse matrices.</description>
    </item>
    
    <item>
      <title>Constraints in the Levenberg-Marquardt least-squares optimization</title>
      <link>https://quantsrus.github.io/post/constraints-in-levenberg-marquardt-least-squares-optimization/</link>
      <pubDate>Mon, 08 Oct 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/constraints-in-levenberg-marquardt-least-squares-optimization/</guid>
      <description>The standard Levenberg-Marquardt optimizer does not support box constraints. This is, for example the case for good MINPACK lmdif/lmder implementations that many optimization libraries use underneath. But in practice, it is often useful to limit the range of the variables, often because the objective might not be defined everywhere.
The R minpack.lm CRAN package allows the user to specify box constraints. How do they do it?
They choose a very straightforward projection approach to enforce the constraints: if the guess is outside the box, they place it at the closest boundary (see the source code).</description>
    </item>
    
    <item>
      <title>The state of open-source quadratic programming convex optimizers</title>
      <link>https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/</link>
      <pubDate>Tue, 24 Jul 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/</guid>
      <description>I explore here a few open-source optimizers on a relatively simple problem of finding a good convex subset, but with many constraints: 30104 constraints for essentially 174 variables. My particular problem can be easily expressed in the form of a quadratic programming problem.
Java  Ojalgo: very difficult to setup properly in the absence of documentation for convex minimization. I suspect it does not work well, as it has a tendency to never finish.</description>
    </item>
    
    <item>
      <title>Staying arbitrage-free with Andreasen-Huge one-step interpolation</title>
      <link>https://quantsrus.github.io/post/staying-arbitrage-free-with-andreasen-huge-volatility-interpolation/</link>
      <pubDate>Thu, 08 Mar 2018 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/staying-arbitrage-free-with-andreasen-huge-volatility-interpolation/</guid>
      <description>Not long ago, I wrote a post about Andreasen-Huge arbitrage-free volatility interpolation method, showing that using a spline for the one-step local volatility instead of a piecewise-constant (or better, a piecewise-linear) function was not necessarily a great idea.
What we get out of Andreasen-Huge method, is a list of discrete option prices. What about option prices for strikes not on the grid?
In this case we still need some kind of interpolation.</description>
    </item>
    
    <item>
      <title>On the Quality of Research Publications</title>
      <link>https://quantsrus.github.io/post/on-the-quality-of-research-publications/</link>
      <pubDate>Wed, 28 Feb 2018 20:56:42 +0100</pubDate>
      
      <guid>https://quantsrus.github.io/post/on-the-quality-of-research-publications/</guid>
      <description>I spent the last week-end to review a paper for the journal Expert Systems with Applications. It was a paper on a variant of Spider Monkey Optimization, which is in the same spirit as differential evolution or particle swarm optimization. Yes, it could be added to the list of esoteric optimizers at the end of this Quants R Us post.
While the manuscript was relatively interesting in itself, and there was definitely some non-trivial amount of work behind it, it was riddled with errors: in the equations, in the algorithms, in the text, in the examples.</description>
    </item>
    
    <item>
      <title>Webassembly still fragile</title>
      <link>https://quantsrus.github.io/post/webassembly_still_fragile/</link>
      <pubDate>Tue, 05 Dec 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/webassembly_still_fragile/</guid>
      <description>As I am preparing the website for my upcoming book on equity derivatives models, I played around with webassembly to run some C++ code from your web browser. In order to do that, I rely on emscripten, which seems to be the most advanced toolkit to generate webassembly code.
I did not expect the webassembly toolchain to be so fragile. At first I had trouble using some specific C code (not so complicated) from javascript through emscripten: there are multiple ways to do it.</description>
    </item>
    
    <item>
      <title>Particle Swarm Optimization on Heston Small-Time Expansion</title>
      <link>https://quantsrus.github.io/post/particle_swarm_optimization_heston_calibration/</link>
      <pubDate>Thu, 06 Jul 2017 07:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/particle_swarm_optimization_heston_calibration/</guid>
      <description>This is a sequel to my previous post on Particle Swarm Optimization. Here, I look at the problem of &amp;ldquo;calibrating&amp;rdquo; a Heston small-time expansion, the one from Forde &amp;amp; Jacquier). This can be useful to find a good initial guess for the exact Heston calibration, computed with much costlier characteristic function Fourier numerical integration.
Unfortunately, as we see below, with the contour plots of the objective function (the RMSE in volatilities against market quotes), the problem is much less well behaved with the small-time expansion than with the numerical integration.</description>
    </item>
    
    <item>
      <title>Particle Swarm Optimization</title>
      <link>https://quantsrus.github.io/post/particle_swarm_optimization/</link>
      <pubDate>Fri, 30 Jun 2017 07:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/particle_swarm_optimization/</guid>
      <description>In my previous post, I looked at simulated annealing to calibrate Heston. A less well-known and more fancy global minimizer is the particle swarm optimization (PSO). I stumbled upon it by accident through a youtube presentation from James McCaffrey. He shows a small python algorithm that solves the travelling salesman problem.
Intrigued, I started to read papers on it. It turns out there are a lot of publications on PSO, not always of great quality.</description>
    </item>
    
    <item>
      <title>Differential evolution vs. Simulated annealing</title>
      <link>https://quantsrus.github.io/post/differential_evolution_vs_simulated_annealing/</link>
      <pubDate>Wed, 21 Jun 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/differential_evolution_vs_simulated_annealing/</guid>
      <description>The differential evolution (DE) algorithm is somewhat popular in quantitative finance, for example to calibrate stochastic volatility models such as Heston. There are a few parameters to setup properly but in general, it is not too difficult to find those, and the algorithm works well on many different problems.
An older technique, much more popular in physics is simulated annealing (SA). There are few papers on its use for stochastic volatility calibration, most don&amp;rsquo;t find the technique competitive (the ASA algorithm appear very slow in this paper from Ricardo Crisostomo, more standard SA is worse than a random search according to ManWo Ng) or even usable (see Jorg Kienitz book).</description>
    </item>
    
    <item>
      <title>A spline to fill the gaps with Andreasen-Huge one-step method</title>
      <link>https://quantsrus.github.io/post/andreasen_huge_spline/</link>
      <pubDate>Thu, 11 May 2017 22:48:01 +0200</pubDate>
      
      <guid>https://quantsrus.github.io/post/andreasen_huge_spline/</guid>
      <description>I recently stumbled upon a blog which suggested to not stay flat with Andreasen-Huge arbitrage free volatility interpolation method. The paper from Andreasen and Huge specifies a piecewise constant (single-step) local volatility where the number of constants matches the number of market option prices.
The blog post shows eventual unstability with the piecewise constant approach, not visible with a linear interpolation. I wondered then if we should not go to the next level: use a spline on N values where N is the number of market options prices.</description>
    </item>
    
  </channel>
</rss>